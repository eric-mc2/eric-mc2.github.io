[{"categories":null,"contents":"This page describes the steps I took to create a panel dataset of ridership across different Chicago transit systems. I did this to practice econometric modeling.\nTransit modes and data sources Included modes\nTrain, Bus, Rideshares - from the City of Chicago (used Socrata API and sodapy python package) Bikeshares aka Divvy Bikes - from Lyft (used s3fs python package to access public Amazon S3 bucket) Excluded modes\nMetra commuter rail (public data is available but too coarsely aggregated) Pace city-suburb bus link (have not checked availability) Walking / Biking (vendored data is available \u0026ndash; ie cell phone location data \u0026ndash; but would be hard to isolate from other transit modalities) Driving (public data is available \u0026ndash; ie road segment usage \u0026ndash; but haven\u0026rsquo;t seriously looked yet) Extra data\nGeographic boundaries - from US Census Bureau (used pygris python package) Landmark boundaries - also from City of Chicago ETL Pipeline Major steps in processing this data:\nExtract Get raw unit-level data (i.e. train stations, bus lines) Get raw ridership time series data Transform Account for schema drift in each data source Merge unit info and time series data Compute and add covariates for regression Merge Merge transit modalities at different spatial and time aggregations Choose Choose best aggregation for regression modeling ETL Illustration Some datasets were better organized than others. The easier ones only required:\nselecting spatial and time range to query reading data documentation figuring out which are the table primary and foreign keys light conversion of data and object types In the simplest of cases, here\u0026rsquo;s all the code we need to query and arrange a panel:\nfrom src.data.cta import (ChiClient, BUS_ROUTES_TABLE, BUS_RIDERSHIP_TABLE) from src.data.gis import WORLD_CRS from shapely.geometry import shape # Query static info about bus routes client = ChiClient(60) bus_routes = client.get_all(BUS_ROUTES_TABLE, select=\u0026#34;the_geom, route, name\u0026#34;) bus_routes = (bus_routes .assign(geometry = bus_routes[\u0026#39;the_geom\u0026#39;].apply(shape)) .drop(columns=\u0026#39;the_geom\u0026#39;) .pipe(gpd.GeoDataFrame, crs=WORLD_CRS)) # Query daily rides per route data_start = \u0026#34;2024-01-01T00:00:00\u0026#34; data_end = \u0026#34;2024-08-31T23:59:59\u0026#34; bus_rides = client.get_all(BUS_RIDERSHIP_TABLE, select=\u0026#34;route,date,daytype,rides\u0026#34;, where=f\u0026#34;date between \u0026#39;{data_start}\u0026#39; and \u0026#39;{data_end}\u0026#39;\u0026#34;) bus_panel = bus_rides.merge(bus_routes, how=\u0026#39;left\u0026#39;, on=\u0026#39;route\u0026#39;) Let\u0026rsquo;s plot these routes, colored by ridership:\nAfter getting the direct transit data, I coded a few extra features such as census tract and population, which could serve as useful regression controls later on.\nimport pygris # CTA routes extend outside chicago into cook cty. tracts = pygris.tracts(state=\u0026#39;IL\u0026#39;, county=\u0026#39;cook\u0026#39;, cb=True, year=2020, cache=False) tracts = tracts[[\u0026#39;GEOID\u0026#39;,\u0026#39;geometry\u0026#39;]] tracts[\u0026#39;GEOID\u0026#39;] = pd.to_numeric(tracts[\u0026#39;GEOID\u0026#39;]) def code_tract(gdf, tracts_gdf) -\u0026gt; pd.Series: \u0026#34;\u0026#34;\u0026#34;Spatial join points to census tract\u0026#34;\u0026#34;\u0026#34; codes = (gdf .filter([\u0026#39;geometry\u0026#39;]) .to_crs(tracts_gdf.crs) .sjoin(tracts_gdf, how=\u0026#39;left\u0026#39;, predicate=\u0026#39;within\u0026#39;)) return pd.concat([gdf, codes[\u0026#39;GEOID\u0026#39;].rename(\u0026#39;tract\u0026#39;), axis=1]) bus_stops = bus_stops.pipe(code_tract) Now we know the immediate population served by each bus line.\nSince ridership is reported at different levels of granularity for each transit modality, I had to be careful how to spatially aggregate the data. My goal is to test whether local events, such as football games, have a measurable effect on transit. Associating transit stations to points of interest is straightforward \u0026ndash; each point is an unambiguous distance away. Associating shapes such as bus routes or census tracts is more ambiguous \u0026ndash; do we measure the distance to the closest bus stop or the furthest? The data doesn\u0026rsquo;t tell us whether riders travel short or far distances along these routes. This ambiguity can create inaccurate regression estimates.\nTrain: provides daily boarding counts per station. we do not know where each passenger exited, nor which direction or train route they took. Bus: provides daily boardings counts per route. we do not know how this breaks down per bus stop, nor the direction or stop passengers exited at. Bike: provides trip-level data: timestamped station pickups and dropoffs per ride. Bike: provides trip-level data: timestamped pickup and dropoffs per ride. pickup and dropoff points are anonymized to their containing census tract. Modality Train Bus Bike Uber Time granularity Daily Daily Minute Minute Direction of travel No No No No Point of departure Yes No Yes No Line of departure Inferred Yes No No Area of departure Yes No Yes Yes Point of arrival No No Yes No Line of arrival Inferred Yes No No Area of arrival Yes No Yes Yes I aggregated three comparable panels:\nPoint: train, bike Line: train, bus Area: train, bike, uber Special Challenges Divvy Locations The divvy historical data spans from 2013 to 2024. Over these years, the published schema has changed several times. This led to annoying, but forgiveable issues to work around:\nBatch Size: Some years are published in monthly batches, others quarterly. This isn\u0026rsquo;t an issue for data modeling, since we just concatenate, but it requires extra logic for properly discovering and enumerating the available data.\nInconsistent column names: Also not an issue for modeling, since I just rename them, but requires manually curating a mapping of {old name -\u0026gt; new name} .\nNormalized vs Denormalized: Some years contain data in \u0026ldquo;database normalized\u0026rdquo; format with separate stations and rides tables, linked by a common station_id key. Other years only contain a single de-normalized/merged rides table, with station info included.\nBut the divvy data contained two way worse problems:\nUnstable station id\u0026rsquo;s\nIn some batches, the station ID is an integer, while in others it is a UUID, representing schema drift. This drift broke the one-to-one property of station IDs: 60% of station IDs were one-to-many: they were associated with up to 4 unique stations.\nFor those not familiar with Chicago streets, these intersections are quite far apart:\nThe standard approach to resolving this issue is to create a column labeling each observation with the batch it came from (eg. \u0026ldquo;2024-Q1\u0026rdquo;) and use (ID, batch) pairs as the new primary key. Although this fixes the one-to-many issue, it breaks the injective property of station IDs, making them many-to-one!\nUnstable station name\nLooking at the data this way, perhaps the station name is a sufficient primary key?\nUnfortunately, station names were also not always one-to-one. For example, the following locations are all purportedly at Buckingham Fountain:\nSimilarly, station names were also not always injective.\nImprecise station locations\nRecent Divvy historical data batches are published as single denormalized CSV\u0026rsquo;s containing trip_start_location and trip_end_location geometries. Maybe the points can be our primary keys?\nAt first glance, there are some ~900,000 unique points. At MOST I should expect a few thousand!\nMaybe this is a floating point precision issue and the points are actually well-clustered around their true station locations. Maybe I can derive a much smaller set of representative locations and snap each point to the closest one.\nAttribute Group-By: I group by attribute (e.g. name, id, batch), compute the centroid of each group, and measure the maximum point-to-centroid distance per group. This quantifies the spread or imprecision in using names, ids, batches as primary keys to actually identify station locations.\nIn code this would look something like:\nTOLERANCE = 1320 # feet def dispersion(x: gpd.GeoSeries, metric: Callable = np.std): # Taking centroid of unique points makes us more sensitive to outlier mis-labeled data when # using standard deviation as metric. When using max it makes no difference. x = x.drop_duplicates() c = MultiPoint(x.values).centroid radius = metric(x.distance(c)) diam = radius * 2 # very rough return diam max_spread = bike_rides.to_crs(LOCAL_CRS) .groupby(\u0026#39;name\u0026#39;)[\u0026#39;geometry\u0026#39;] .transform(lambda x: dispersion(x, np.max)) clusterable = max_spread \u0026lt; TOLERANCE Based on this method, ~106,000 points can be reduced down to ~1,00 representative points, but the remaining ~786,000 points are too dispersed to confidently say the refer to the same point.\nSpatial Clustering: As seen above, these ID columns are not necessarly bijective, meaning they are over-specified. Maybe I should group the points purely by their location, and ignore their IDs.\nThere are many ways to cluster points. I\u0026rsquo;ll use an intuitive way as follows:\nDraw a buffer around each point (66ft is the typical Chicago street width) Find all buffers that intersect (via unary union) Each unioned buffer shape represents a candidate cluster Build spatial index on these clusters For each point, determine which cluster it belongs to (via spatial index query) Break apart clusters that are: Too dispersed Have heterogenous attributes (e.g. station names) This method affirms the outcome of the attribute clustering performed above. The 20% of data that is tightly spatially grouped do indeed represent unique stations. The other 80% of the data can be unioned into clusters that contain up to 7818 points.\nThe buffer union method suffers from a transitivity issue:\n$$ \\text{dist}(A,B) \u0026lt; d, \\text{dist}(B,C) \u0026lt; d \\not \\Rightarrow \\text{dist}(A,C) \u0026lt; d $$\nWe can see that step 5 helps mitigate this issue.\nI have to conclude that these points actually represent user locations when they start/stop trips on their smartphone apps. The drift represents a combination of GPS uncertainty among tall downtown buildings, and physical user displacement from the actual station when they interact with the app.\nResolution with GBFS\nAs a last resort, I looked at and decided to merge the live GBFS feed. This dataset is intended for app developers, and shows real-time station availability and such. The problem is it\u0026rsquo;s live and doesn\u0026rsquo;t span the entire historical data range. Given the schema drift issues presented above, I had no confidence the data was merge-able with anything but 2024 data. But since I only strictly need 2024 data for the models I plan to run, this limitation is acceptable.\nProblem solved!\nSocrata to Data Frames As far as I could tell, the Socrata SoQL API returned all data as lists of lists of strings. Since I was making a lot of Socrata calls, I extended the Socrata client like so:\nfrom sodapy import Socrata import pandas as pd class ChiClient(Socrata): def __init__(self, timeout: int): super().__init__( \u0026#34;data.cityofchicago.org\u0026#34;, app_token=None, timeout=timeout) def get(self, resource_id: str, **params) -\u0026gt; pd.DataFrame: \u0026#34;\u0026#34;\u0026#34;Collects Socrata response into data frame.\u0026#34;\u0026#34;\u0026#34; data = super().get(resource_id, **params) df = pd.DataFrame.from_records(data) df = self.fix_coltypes(df, resource_id) return df def fix_coltypes(self, df: pd.DataFrame, resource_id: str): \u0026#34;\u0026#34;\u0026#34;Coerce string data into other dtypes\u0026#34;\u0026#34;\u0026#34; coltypes = self.get_coltypes(resource_id) for col in df.columns: if col not in coltypes.keys(): # Column was transformed e.g. SELECT COUNT(col) # or renamed e.g. SELECT col AS alias continue elif coltypes[col] == \u0026#39;calendar_date\u0026#39;: df[col] = pd.to_datetime(df[col]) elif coltypes[col] == \u0026#39;number\u0026#39;: df[col] = pd.to_numeric(df[col]) return df def get_coltypes(self, resource_id: str): \u0026#34;\u0026#34;\u0026#34;Query basic table metadata\u0026#34;\u0026#34;\u0026#34; meta = self.get_metadata(resource_id) colnames = [c[\u0026#39;fieldName\u0026#39;] for c in meta[\u0026#39;columns\u0026#39;]] coltypes = [c[\u0026#39;dataTypeName\u0026#39;] for c in meta[\u0026#39;columns\u0026#39;]] coltypes = {c: ct for c,ct in zip(colnames, coltypes)} return coltypes Usage comparison:\nIn [1]: TOTAL_RIDERSHIP_TABLE = \u0026#34;6iiy-9s97\u0026#34; In [2]: soc_client = Socrata(\u0026#34;data.cityofchicago.org\u0026#34;, app_token=None, timeout=60) In [3]: chi_client = ChiClient(60) In [4]: original_data = soc_client.get(TOTAL_RIDERSHIP_TABLE, limit=5) In [5]: pretty_data = chi_client.get(TOTAL_RIDERSHIP_TABLE, limit=5) In [6]: print(\u0026#34;Original response:\u0026#34;, original_data, sep=\u0026#34;\\n\u0026#34;) Original response: [{\u0026#39;service_date\u0026#39;: \u0026#39;2001-01-01T00:00:00.000\u0026#39;, \u0026#39;day_type\u0026#39;: \u0026#39;U\u0026#39;, \u0026#39;bus\u0026#39;: \u0026#39;297192\u0026#39;, \u0026#39;rail_boardings\u0026#39;: \u0026#39;126455\u0026#39;, \u0026#39;total_rides\u0026#39;: \u0026#39;423647\u0026#39;}, {\u0026#39;service_date\u0026#39;: \u0026#39;2001-01-02T00:00:00.000\u0026#39;, \u0026#39;day_type\u0026#39;: \u0026#39;W\u0026#39;, \u0026#39;bus\u0026#39;: \u0026#39;780827\u0026#39;, \u0026#39;rail_boardings\u0026#39;: \u0026#39;501952\u0026#39;, \u0026#39;total_rides\u0026#39;: \u0026#39;1282779\u0026#39;}, {\u0026#39;service_date\u0026#39;: \u0026#39;2001-01-03T00:00:00.000\u0026#39;, \u0026#39;day_type\u0026#39;: \u0026#39;W\u0026#39;, \u0026#39;bus\u0026#39;: \u0026#39;824923\u0026#39;, \u0026#39;rail_boardings\u0026#39;: \u0026#39;536432\u0026#39;, \u0026#39;total_rides\u0026#39;: \u0026#39;1361355\u0026#39;}, {\u0026#39;service_date\u0026#39;: \u0026#39;2001-01-04T00:00:00.000\u0026#39;, \u0026#39;day_type\u0026#39;: \u0026#39;W\u0026#39;, \u0026#39;bus\u0026#39;: \u0026#39;870021\u0026#39;, \u0026#39;rail_boardings\u0026#39;: \u0026#39;550011\u0026#39;, \u0026#39;total_rides\u0026#39;: \u0026#39;1420032\u0026#39;}, {\u0026#39;service_date\u0026#39;: \u0026#39;2001-01-05T00:00:00.000\u0026#39;, \u0026#39;day_type\u0026#39;: \u0026#39;W\u0026#39;, \u0026#39;bus\u0026#39;: \u0026#39;890426\u0026#39;, \u0026#39;rail_boardings\u0026#39;: \u0026#39;557917\u0026#39;, \u0026#39;total_rides\u0026#39;: \u0026#39;1448343\u0026#39;}] In [7]: print(\u0026#34;DataFrame response:\u0026#34;, pretty_data, sep=\u0026#34;\\n\u0026#34;) DataFrame response: service_date day_type bus rail_boardings total_rides 0 2001-01-01 U 297192 126455 423647 1 2001-01-02 W 780827 501952 1282779 2 2001-01-03 W 824923 536432 1361355 3 2001-01-04 W 870021 550011 1420032 4 2001-01-05 W 890426 557917 1448343 ","permalink":"https://eric-mc2.github.io/projects/dnc-transit/","tags":["data science","open data","Socrata","Amazon S3","geopandas","panel data"],"title":"Chicago Transit Ridership Panel"},{"categories":null,"contents":"What is the scale and structure of private donations to police departments?\nPrevious studies have focused on police foundations moving tens of millions of dollars. Using a unique data license from Guidestar Candid—all nonprofit organization tax returns from 2014-2019—we describe a larger and integrated world of organizations moving private money to police. We introduce the concept of police finance organizations, private entities that channel material resources in support of police departments without facing the same levels of public accountability, regulations, and transparency as the police departments themselves. Our exploratory data analysis and social network methods revealed three types of these organizations: 1) connectors, 2) boosters, and 3) havens. Connectors donated to multiple police departments. Boosters donated exclusively to one police department. Havens publicly and materially supported police, yet did not officially report doing so. Overall, we provide the most complete description of the vast and underestimated structure of private donations.\n","permalink":"https://eric-mc2.github.io/publications/shachter-structure/","tags":["social media analysis","network analysis","SNA","NLP"],"title":"The Social Structure of Private Donations to Police"}]