[{"categories":null,"contents":"This post describes the technical implementation of the public safety news analysis.\nResearch Question What is the spatial distribution of crime news coverage across Chicago?\nI decompose this question into smaller machine learn-able tasks:\nIs the article crime-related? (classification) Which sentences in the article describe the crime incident? (classification) Which words refer to a location? (named entity recognition) Which location words relate to the crime incident? (dependency parsing) Which neighborhood is this in? (classification) Data Sources News articles: The data is a sample of 713k articles from the Chicago Justice Project\u0026rsquo;s news archive from 201X onwards.\nNews articles serve as training data for all five model stages.\nChicago geographies: Downloaded community area boundaries, neighborhood boundaries, street network from City of Chicago data portal.\nGeometries are used for exact matching and classification when possible. They are also used to generate synthetic training data for neighborhood classification.\nModels Article Classifier: I classify articles as crime-related or not, based off of the article title, using Spacy\u0026rsquo;s off-the-shelf binary text classifier (for the proof of concept, a unigram BOW).\nArticles tagged as non-crime-related are passed through the rest of the pipeline with no further processing.\nSentence Classifier: I classify whether each sentence from the article body describes the WHO, WHAT, HERE, or WHEN of the crime incident, using Spacy\u0026rsquo;s off-the-shelf multi-label text classifier (for the proof of concept, a unigram BOW).\nLocation Recognition: I created a custom Spacy Pipeline component for NER:\nUse Spacy\u0026rsquo;s PhraseMatcher to efficiently check for exact matches against a list of all Chicago street names, intersections, address blocks, neighborhoods, community areas, and \u0026ldquo;sides\u0026rdquo; (e.g. North Side). Run Spacy\u0026rsquo;s pre-trained (tok2vec) NER model to identify more GPE (counties, cities) and FAC (highways, airports) entities. Clean up and merge entities. Tag entity with fine-grained location type, based on previous matching method. This combination of rules-based and machine-learning consistently fixes common cases missed by the model, e.g. consolidating \u0026ldquo;(1300)[number] block of (North Webster Ave)[street]\u0026rdquo; into \u0026ldquo;(1300 block of North Webster Ave)[street]\u0026rdquo;.\nDependency Parsing Not implemented yet. For the proof of concept, I include sentences that were positive matches for both sentence classification and location entity recognition.\nSentences that were negative matches for either stage are passed through the rest of the pipeline with no further processing.\nCommunity Area Classification\nPredict neighborhood using Spacy\u0026rsquo;s off-the-shelf multi-class text classification model (for proof of concept, bi-gram BOW). The training data for this model is half hand-labeled, half synthetic data sampled from the city geometries. Tag blocks, intersections, and neighborhoods with their known community area, based on their spatial relationships. Merge labels from machine learning and exact methods. Infrastructure I\u0026rsquo;m using Dagster to explicitly declare and manage the separate pre-processing, training, evaluation, and tuning steps per pipeline stage.\nPipeline I have a bias towards file-centric pipeline declarations like Makefiles and dvc, particularly the way these tools abstract where data is located away from the code operating on the data.\nData Discovery My goal is to make it easy to quickly scan the order that data files are created and the relationships between inputs, processing scripts, and outputs. First, I define all data paths in an external config file, which is organized by stage:\n# pipeline.yaml raw: article_text: \u0026#34;raw/articles.parquet\u0026#34; art_relevance: article_text_labeled: \u0026#34;art_relevance/articles_labeled.jsonl\u0026#34; article_text_train: \u0026#34;art_relevance/articles_train.spacy\u0026#34; article_text_test: \u0026#34;art_relevance/articles_test.spacy\u0026#34; sent_relevance: article_text_labeled: \u0026#34;sent_relevance/articles_labeled.jsonl\u0026#34; article_text_train: \u0026#34;sent_relevance/articles_train.spacy\u0026#34; article_text_test: \u0026#34;sent_relevance/articles_test.spacy\u0026#34; These paths are relative, allowing the data folder itself to be environment-specific, which is checked when the config is loaded. (For local development it is just \u0026ldquo;./data\u0026rdquo; within the project folder. For running on Colab, it is a Google Drive path \u0026ldquo;/gdrive/MyDrive/\u0026hellip;/data\u0026rdquo;).\n# script.py from scripts.utils import Config config = Config() print(config.get_data_path(\u0026#34;raw.article_text\u0026#34;)) \u0026#34;/absolute/path/to/project/data/raw/newsarticles.parquet\u0026#34; Stage Declarations My goal is to make this file really compact and to highlight a) the logical stage dependencies and b) the input output flows. To achieve this, I separate the actual processing logic to a different file operations.py. And as before, the actual data locations are isolated and loaded through the config file.\n# scripts/preprocessing/assets.py import dagster as dg from scripts.utils import Config from scripts.preprocessing import operations as ops config = Config() @dg.asset def extract(description=\u0026#34;Unzip raw data\u0026#34;): in_path = config.get_data_path(\u0026#34;raw.zip\u0026#34;) out_path = config.get_data_path(\u0026#34;raw.article_text\u0026#34;) ops.extract(in_path, out_path) # scripts/art_relevance/assets.py import dagster as dg from scripts.utils import Config from scripts.art_relevance import operations as ops config = Config() @dg.asset(deps=[extract], description=\u0026#34;Split labeled data for training.\u0026#34;) def split_train_test(): in_path = config.get_data_path(\u0026#34;art_relevance.article_text_labeled\u0026#34;) out_path_train = config.get_data_path(\u0026#34;art_relevance.article_text_train\u0026#34;) out_path_test = config.get_data_path(\u0026#34;art_relevance.article_text_test\u0026#34;) ops.split_train_test(in_path, out_path_train, out_path_test) This may look like boilerplate, but I like it\u0026rsquo;s cleanliness. The logical separation makes it very clear that assets.py is about defining arbitrarily complicated Dagster dependencies, whereas operations.py is about defining arbitrarily complicated data manipulation.\nTraining Annotation My goal is to prototype each sub-model quickly, get a sense of the overall performance, get a sense of which model need improvement. I sample an hour\u0026rsquo;s worth of articles per model, and annotate them in Label Studio.\nA positive example.\nA negative example.\nTuning I am using MLFlow to track model experiments and Optuna for hyperparameter tuning. I load the spacy config file, flatten it like {\u0026quot;nested.key.param1\u0026quot;: val1, \u0026quot;nested.key.param2\u0026quot;: val2} and dump the full parameter list into MLFlow. This way I\u0026rsquo;m never missing a baseline parameter value if I run a new experiment, even though 99.9% of the values are unchanged run-to-run.\nSo far, hyper-parameters are not significantly altering performance. This is likely due to my small sample size (200-400 examples). The validation set is so tiny (holding out 10%), there isn\u0026rsquo;t enough variation in it for the models to predict differently.\nI compare the classification models to a \u0026ldquo;null model\u0026rdquo;: randomly predict the positive class according to the frequency of positive values in the training set.\nDownstream Models In production, the sentence classifier will only see sentences from articles which the article classifier has labeled as \u0026ldquo;crime-related\u0026rdquo;. In other words, its data distribution is conditional on the article classifier. To mirror this aspect during training, I take an independent sample from the raw data and pass it through the article classifier to filter relevant articles. I pre-process the article text, including breaking it into sentences. And annotate the sentences by hand in Label Studio.\n","permalink":"https://eric-mc2.github.io/projects/qjn-methods/","tags":["machine learning","data engineering","spacy","nlp","dagster","mlflow","GIS"],"title":"Public Safety News Analysis (Engineering)"},{"categories":null,"contents":"Introduction According to Pew Research, crime is the second-most watched news topic besides weather. Over three quarters of U.S. adults say get news about crime sometimes or often.1 How accurate is this information? How does it affect public perceptions on crime? And how, ultimately, does that affect public safety conversations, economic behaviors, and social good.\nDecades of social science research have shown the importance of news coverage in shaping our world views. The media helps determine what issues are important and deserve attention, a process called \u0026ldquo;scheduling\u0026rdquo;. Within each story, journalists also decide what aspects to focus on, showing readers how to make meaning of this information, and \u0026ldquo;framing\u0026rdquo; the story in a particular set of values.2\nPerceptions of crime Decades of opinion research have also shown a consistent disconnect between fear of crime victimization and actual crime rates. For the past 29 out of 31 years, a majority of U.S. adults have said that crime is worse this year than last year.3 This is despite official statistics showing consistent decreases in violent crime rates over time.\nOverall, fear of victimization has remained a salient issue, with 46% of adults describing crime in their area as being an extremely to somewhat serious problem. These perceptions cut across types of crime: 69% of households report that they frequenty or occasionally worry about identity theft, 30% worry about muggings, 21% worry about sexual assault.\nExperience with crime Violent crimes are overall much rarer than property crimes. Comparatively, certain white-collar and cyber-crimes are surprisingly ubiquitous. Three percent of households report being victims of physical assault or mugging, and 2% of sexual assault. On the other hand, 13% percent report being victims of vandalization, whereas 24% of households have report being victims of credit card hacking, 15% of financial scams, 14% of identity theft.\nPerceptions of crime have immediate effects on behavior, with longer term systemic implications carrying over to other policy realms. For instance, almost half of U.S. adults claim to avoid certain places or neighborhoods due to concern over crime and 32% report having bought a gun for protection.3\nAccuracy of media coverage Survey after survey consistently show a positive correlation between percieved victimization risk and news consumption.4 Whether one causes the other, or vice versa, is an empirical question. 50% of adults say the media neither over nor under reports crime. Given this link, it is important to investigate, does the coverage of crime reflect actual crime rates? Are locations and types of crimes accurately represented?\nThe Chicago Justice Project has previously highlighted disparities in stranger vs intimate partner violence rates compared to the types of cases that are covered.\nLiterature Review Previous studies have relied mostly on opinion surveys. Succar et. al. analyze Twitter streams by counting the number of tweets mentioning crime-specific keywords, and performing a sentiment analysis on police/crime-related tweets.5\nThe Quantify Justice News project extends their methods in two ways. First, we analyze the content of news articles, specifically the locations of reported crimes. Second, we train a machine learning model to classify articles as crime-related, rather than relying on exact keyword matches.\nMethods Using a database of daily local news articles covering the Chicagoland area, I trained a machine learning pipeline to analyze:\nWhat locations (community areas) covered in news of crime?\nWhy community areas? For privacy concerns, crime events are not published with exact addresses. Community areas are stable administrative boundaries designed for long-term policy analysis and planning.\nML Tasks Following this excellent talk from the folks at Spacy I\u0026rsquo;ve broken down this research question into simpler sub-tasks that are easier to verify and easier to train a ML model to do.\nIs the article crime-related?6 (classification) Which sentences in the article describe the crime incident? (classification) Which words refer to a location? (named entity recognition) Which locations relate to the crime incident? (dependency parsing) Which neighborhood is this in? (classification) In graphical form:\nflowchart LR text@{ shape: docs, label: \u0026#34;article text\u0026#34;} texts@{ shape: docs, label: \u0026#34;sentences\u0026#34;} label@{ shape: lin-doc, label: \u0026#34;Community area(s)\u0026#34; } discardA([discard]) discardB([discard]) style RELA fill:#FFF,stroke-width:4px,stroke:#000,stroke-dasharray:2 2 subgraph RELA[Relevance Model] CrimeA@{ shape: diamond, label: \u0026#34;Crime-related article?\u0026#34; } end style RELS fill:#FFF,stroke-width:4px,stroke:#000,stroke-dasharray:2 2 subgraph RELS[Relevance Model] CrimeB@{ shape: diamond, label: \u0026#34;Crime incident-related sentence?\u0026#34; } end style DS fill:#FFF,stroke-width:4px,stroke:#000,stroke-dasharray:2 2 subgraph DS[NLP/Geo Models] NER CLF end text --\u0026gt; CrimeA CrimeA --\u0026gt; |No| discardA CrimeA --\u0026gt; |Yes| texts texts --\u0026gt; CrimeB CrimeB --\u0026gt; |No| discardB CrimeB --\u0026gt; |Yes| NER NER --\u0026gt; CLF CLF --\u0026gt; label For a more in-depth discussion of this model and implementation, see my next post.\nResults Coming Soon!\nhttps://www.pewresearch.org/short-reads/2024/08/29/the-link-between-local-news-coverage-and-americans-perceptions-of-crime/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://doi.org/10.56238/sevened2023.006-152\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://news.gallup.com/poll/1603/Crime.aspx\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://link.springer.com/article/10.1007/s10940-015-9261-x\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.nature.com/articles/s44271-024-00059-8\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis imposes a strong framing on the analysis. Why crime news instead of all public-safety news (ie. accidents, road closures, changes in police department policy)? What types of crime (e.g. labor and environmental violations)?\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://eric-mc2.github.io/projects/qjn/","tags":["crime","media analysis","nlp"],"title":"Public Safety News Analysis"},{"categories":null,"contents":"Recap In a previous post I proposed a statistical model to estimate whether the 2024 Chicago DNC affected public transit usage across Chicago.\nThe learning objective was to was to try to construct a believable model from a real-life event \u0026ndash; or to convince myself that the model is too flawed to be trustworthy.\nI gathered ridership data for train, bike, and rideshares, plus the dates and locations of the DNC. Then I estimated two regression models: a fixed-effects model and a difference-in-difference model. The results indicate that the DNC caused ridership to increase near the convention centers. However, mobility across the rest of Chicago also declined during that week.\nDo we belive this design? I\u0026rsquo;ll list several issues that I thought of.\nProblems with this model (Key: üò° = dealbreaker; üôÅ = bad but maybe fixable; ü´§ = not a dealbreaker)\nControl Mis-Specification - üò°\nthe control group should be \u0026ldquo;like\u0026rdquo; the treatment group, but I use the entire city as a comparison. it\u0026rsquo;s not apples-to-apples. a better control group might be a handful of the major event/cultural institutions like Soldier Field, Wrigley Field, Art Institute. or using finding matched pairs based on covariates. Selection Bias - üò°\nthe convention centers were explicitly chosen for their ability to accomodate visitors (via transit) this is mostly a problem for external validity (over-estimating the result if repeated elsewhere), which is not the point here Substitutability of Transit - üôÅ\nTravelers have different options for transit. Their choice depends on location, time of day, distance cost and reliability of other options. I don\u0026rsquo;t like that I\u0026rsquo;m modeling these modes independently right now, but unifying them would be a rabbit hole. Spillover into control - üôÅ\nDNC visitors might stay in chicago longer than the official convention dates, which would attenuate any effect we can mitigate this by using placebo dates or by an event study design DNC visitors may visit other parts of the city, attenuating the effect we can mitigate this by maybe searching for spikes in other areas (but introduces a multiple testing issue) Since the effects were statistically significant, I am not worried about these attenuation biases.\nSpillover into treatment - üôÅ\ndrawing buffers around the event centers weakens our identification strategy (marginally nearby transit might be spuriously related to the DNC itself) This concern can be mitigated (for tract-level model) with a robustness check, by varying whether the buffer must contrain 100% of tract land area, 75%, 50%, etc.\nConfounding Variables - üôÅ\nthe security perimeter around the event centers may actually suppress ridership It will be hard to disentagle the security effect (-) from the DNC effect (+). One way might be to test the sensitivity of the model to varying buffer sizes (smaller than the perimeter, equal to the perimeter, larger than the perimeter).\nGravity and Catchement Models - ü´§\nTheoretically I\u0026rsquo;m interested in true origins and destinations, not the \u0026ldquo;first/last stop\u0026rdquo; of transit, which is the data I have. For rideshares, the two are probably the same. But for trains and bikes we can assume people need to walk the last mile, which may mean crossing census tract boundaries. This is why I haven\u0026rsquo;t aggregated station-level ridership to tract level. I\u0026rsquo;d want to model some cross-tract spillover e.g. via a gaussian. Fixing the Selection Problem Matched Pairs One way to mitigate selection bias is to choose control units that are \u0026ldquo;like\u0026rdquo; treatement units\u0026rsquo; at baseline. I can model\n$$ P(\\text{near convention} | X) $$ and then find other units with high probabilities that were in fact not near the DNC. Unfortunately, I don\u0026rsquo;t have a strong theoretical definition of \u0026ldquo;likeness\u0026rdquo;, nor the data to measure it. Ideally I\u0026rsquo;d like to operationalize \u0026ldquo;ability to handle large crowds\u0026rdquo;. I didn\u0026rsquo;t see maximum fire code capacity in Chicago\u0026rsquo;s building footprint dataset. But I can measure attendance at crowded events.\nAttendance Model I\u0026rsquo;ll compare the DNC locations (United Center and McCormick Place) to Chicago\u0026rsquo;s other major event venues. I chose Wrigley Field, Guaranteed Rate Field, and Soldier Field because per-game sports attendance data is readily available1. I also pull in conference event data for McCormick Place2.\nNow the control group is more \u0026ldquo;similar\u0026rdquo; in terms of its transit patterns. I\u0026rsquo;ll include event attendance as a variable in the regression.\nOne drawback of this method is that I can only now compare \u0026ldquo;event days\u0026rdquo;, drastically reducing the sample size. Worse, Soldier Field and Guaranteed Rate Field do not have games during the DNC, reducing the active control group just to transit options near Wrigley Field.\nFig: Event timeline. The sample sizes are just too small.\nStadium Model On second thought, using attendance as a regression variable makes it hard to interpret our treatment effect. Ceteris peribus, I\u0026rsquo;d be modeling the effect of the DNC in excess of the DNC attendees. That\u0026rsquo;s not at all what I want.\nWhy not drop the attendance term, but keep the reduced sample of transit near stadiums on all game/non-game days. Now the treatment and control are much more similar in terms of transit density:\nNot Near DNC Near DNC P-Value train stations 12 8 station-days 732 468 daily rides, mean (SD) 3098.3 (2692.9) 2093.5 (1985.5) \u0026lt;0.001 log(daily rides), mean (SD) 7.7 (0.9) 7.0 (1.8) \u0026lt;0.001 bus_distance, mean (SD) 230.0 (340.2) 165.3 (167.7) 0.580 bike_distance, mean (SD) 232.1 (164.3) 288.5 (281.6) 0.620 sqrt(area), mean (SD) 1919.2 (1141.3) 3123.8 (3564.8) 0.382 lat, mean (SD) 0.4 (1.2) -0.3 (0.3) 0.073 long, mean (SD) -0.1 (0.9) -0.8 (1.1) 0.134 \u0001 bike docks 75 47 dock-days 3986 2846 daily rides, mean (SD) 84.1 (64.9) 66.7 (55.9) \u0026lt;0.001 log(daily rides), mean (SD) 4.0 (1.2) 3.9 (0.9) \u0026lt;0.001 train_distance, mean (SD) 1232.8 (1050.7) 1056.5 (1240.4) 0.420 bus_distance, mean (SD) 140.0 (145.4) 105.6 (73.5) 0.087 sqrt(area), mean (SD) 603.4 (215.9) 734.0 (711.5) 0.227 lat, mean (SD) 0.3 (1.3) -0.4 (0.4) \u0026lt;0.001 long, mean (SD) 0.0 (0.8) -0.2 (1.2) 0.179 \u0001 uber tracts 64 40 tract-days 3877 2428 daily rides, mean (SD) 746.7 (2018.7) 927.5 (1896.5) \u0026lt;0.001 log(daily rides), mean (SD) 5.5 (1.5) 5.7 (1.5) \u0026lt;0.001 train_distance, mean (SD) 2010.3 (1149.6) 2293.7 (1027.7) 0.195 bus_distance, mean (SD) 567.4 (324.9) 503.0 (313.2) 0.317 bike_distance, mean (SD) 840.2 (357.8) 889.5 (504.9) 0.592 sqrt(area), mean (SD) 608.4 (254.8) 784.4 (320.0) 0.004 lat, mean (SD) 0.5 (1.2) -0.3 (0.4) \u0026lt;0.001 long, mean (SD) -0.0 (0.7) -0.5 (1.2) 0.028 I estimate the same difference in difference model as before:\n$$ \\log{rides_{it}} \\sim \\beta_0 + \\beta_1 \\text{DNC}_t + \\beta_2 \\text{near DNC}_i + \\beta_3 \\text{DNC}_t \\text{near DNC}_i + X_{it} + u_{it} $$ DiD (Uber) DiD (Train) DiD (Bike) Near DNC 0.6760* 0.1569 -0.0082 (0.3960) (0.4565) (0.1781) During DNC -0.1139*** -0.1447 -0.0957 (0.0412) (0.0980) (0.0737) Near DNC:During DNC 0.2746*** 0.8982 0.4492*** (0.0854) (0.6198) (0.1048) log(dist to train) -0.2957 0.1952*** (0.1995) (0.0526) log(dist to bike) 0.1554 0.5530 (0.3104) (0.3475) log(dist to bus) 0.2676* -0.0356 -0.0679 (0.1437) (0.1728) (0.1009) R-squared 0.5182 0.4228 0.4733 R-squared Adj. 0.5170 0.4159 0.4722 N 6718.0 1280.0 7026.0 In the control group we observe a (non-causal) -10.8%, -13.5% (NS), -9.1% (NS) percentage-point change in rideshare, train, and bike rides, which agrees directionally with the un-subsetted data. The causal effect of the DNC on rideshares, train, and bike rides near the DNC is a +31.6%, +145.5% (NS), and +56.7% percentage-point change, an increase in magnitude.\nAs a robustness check on this model, I plot the parallel trends and run a placebo test, shifting the simulated treatment period back across 8 x 4-day windows. 38% of the simulated models returned statistically significant main effects, but the actual DNC effects (x\u0026rsquo;s) were much larger than the simulated effects (box \u0026amp; whisker).\nFig: Placebo Test. Conclusion I attempt to overcome selection effect bias by a pseudo-matched pairs method, comparing transit ridership near large event centers in Chicago. The results agree directionally and give credence to the original experimental design.\nFootnotes CSV\u0026rsquo;s downloaded from the sports-reference family of websites.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nScraping events listed on tradefest.io\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://eric-mc2.github.io/projects/dnc-attendance/","tags":["data science","econometrics","panel data"],"title":"DNC Transit Effect? (Part 3)"},{"categories":null,"contents":"This research project asks:\nDoes the exogeneous shock of the 2024 Democratic National Convention in Chicago induce significant impacts on public transit usage?\nIn a previous post, I collected Chicago open transit data and designed a quasi-experiment to measure the causal impact of the DNC on ridership.\nRegression Now for the results \u0026hellip;\nFixed Effects Model The fixed effects specification conveniently eliminates static sources of bias without needing to explicitly measure each and every static variable (in other words it controls for all time-invariant unobserved factors). If we believe we have controlled for dynamic factors, such that all other time-varying factors are uncorrelated with the treatment variable, then the FE model gives us an unbiased causal estimate of the treatment effect.\nThe equation includes an indicator for \u0026ldquo;during DNC\u0026rdquo;, day-of-the-week dummies, time trend, and unit fixed effects:\n$$ \\log(\\text{rides}_{it}) \\sim \\beta_0 + \\beta_1\\text{DNC}_t + \\beta_2 \\text{dotw}_t + \\beta_3 t + \\alpha_i $$ The convenience of FE is also a downside: many helpful covariates, including the treatment \u0026ldquo;indicator\u0026rdquo;, distance to DNC, do not vary with time. These factors get subsumed in the unit fixed effect term \\(\\alpha_i\\), meaning we can\u0026rsquo;t estimate them separately. Without being able to isolate the location of the DNC, this model only speaks to city-wide effects during the DNC.\nWith this major caveat in mind, let\u0026rsquo;s run the model (separate models per transit mode1):\nFE (Uber) FE (Train) FE (Bike) During DNC -0.0524*** -0.0814*** 0.0297*** (0.0066) (0.0148) (0.0069) time trend 0.0013*** 0.0014*** -0.0000 (0.0001) (0.0002) (0.0001) Tuesday 0.1059*** 0.1098*** 0.0274*** (0.0047) (0.0105) (0.0051) Wednesday 0.2156*** 0.1172*** 0.1199*** (0.0047) (0.0105) (0.0051) Thursday 0.3314*** 0.0893*** 0.0794*** (0.0047) (0.0105) (0.0051) Friday 0.5977*** 0.0530*** 0.1148*** (0.0047) (0.0106) (0.0051) R-squared 0.9505 0.9019 0.9470 R-squared Adj. 0.9497 0.9003 0.9458 N 64229.0 7910.0 49697.0 The results show a -5.1%, -7.8%, and +3.0% percentage point change in uber, train, and bike ridership during the DNC compared to other summer days. This supports the hypothesis that the anticipated traffic suppressed city-wide mobility (except for bikes which are less affected by traffic).\nDifference in Difference Like fixed effects, the difference in difference model removes bias of time-invariant unobserved factors. Unlike fixed effects, we can include time-invariant observed factors into the model. Additionally, diff-in-diff removes bias from dynamic factors that follow common time-trends between treatment and control groups.\nThe difference in difference model isolates both the area and time of the DNC. First it compares pre/post among the control group.\n$$\\Delta \\text{not nearby} = E(\\text{rides} | \\text{not nearby}, \\text{during DNC}) - E(\\text{rides} | \\text{not nearby}, \\text{not during DNC})$$ Then it compares pre/post among the treatment group.\n$$\\Delta \\text{nearby} = E(\\text{rides} | \\text{nearby}, \\text{during DNC}) - E(\\text{rides} | \\text{nearby}, \\text{not during DNC})$$ Finally it takes the difference of these comparisons: the change in treatment group vs the change in control group.\n$$\\text{DiD} = \\Delta \\text{nearby} - \\Delta \\text{not nearby} $$ In the formal2 model I include time trend, day-of-the-week dummies, distance to nearest transit, and quadratic lat/long terms3.\n$$ \\log{rides_{it}} \\sim \\beta_0 + \\beta_1 \\text{DNC}_t + \\beta_2 \\text{nearby}_i + \\beta_3 \\text{DNC}_t \\text{nearby}_i + X_{it} + u_{it} $$ DiD (Uber) DiD (Train) DiD (Bike) Near DNC 0.7979*** -0.7937 0.4222*** (0.2357) (0.5147) (0.1244) During DNC -0.0666*** -0.1221*** -0.0335 (0.0143) (0.0146) (0.0240) Near DNC:During DNC 0.1845** 0.8550 0.2998*** (0.0810) (0.5248) (0.0555) log(dist to train) -0.3231*** -0.1238*** (0.0563) (0.0215) log(dist to bike) -0.2997*** 0.2345 (0.0553) (0.1478) log(dist to bus) -0.0108 -0.0730 0.0236 (0.0450) (0.0667) (0.0275) R-squared 0.5109 0.2943 0.6597 R-squared Adj. 0.5108 0.2929 0.6596 N 64231.0 7910.0 49704.0 These results show a baseline difference in transit usage between convention and non-convention areas. In the control group we observe a (non-causal) -6.4%, -11.5%, -3.3% (NS) percentage-point change in rideshare, train, and bike rides, which agrees directionally with the fixed-effects model. The causal effect of the DNC on rideshares, train, and bike rides near the DNC is a +20.3%, +135.1%, and +35.0% percentage-point change.\nParallel Trends The diff-in-diff model operates on the \u0026ldquo;parallel trends\u0026rdquo; assumption: the control group and (unobserved) counterfactual treatment group must have similar slopes during the treatment period. In practice, since we cannot observe the counterfactual group, we check whether the treatment and control groups have similar slopes prior to treatment. And we ask \u0026ldquo;do we believe this relationship would hold in the next period\u0026rdquo;?\nPlotting pre-treatment rides, the group trends are very consistent in the long-term and even show similar patterns on a weekly scale. We also don\u0026rsquo;t see any anticipatory effects leading up to the DNC event.\nFig: Parallel trends assumption. Conclusion Areas near the DNC experience +20.3%, +135.1%, and +35.0% higher ridership (for rideshares, train, and bikes) compared to non-convention areas due to the DNC. This amounts to roughly4 24k, 62k, and 3k extra rides.\nWe also observe that ridership in non-convention areas fell during the DNC by -6.4%, -11.5%, and +3.3% compared to their pre-DNC baseline. A loss of roughly 43k, 143k, and 3k rides. This loss cannot be interpreted as the causal result of the DNC in particular.\nOverall the evidence suggests that DNC attendees did in fact use and prefer public transit during their stay in Chicago. However, the event itself (and anticipated traffic) may have suppressed mobility by an even greater extent across Chicago.\nThough we can easily measure what happened during the DNC, the counterfactual without the DNC is unknowable. Counterfactuals are important in policy because they provide a second-best outcome upon which to base our decision (hosting the DNC vs not hosting). These quasi-experimental methods, Fixed Effects and Difference-in-Difference regression, make a principled estimate of the counterfactual outcome, providing a point of comparison for the effect of the DNC itself.\nIn the next post, I list the prevailing issues with my regression design and propose an adjustment to compellingly overcome the worst issue.\nFootnotes See first footnote.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nEstimated with weighted least squares (weighted by number of observations per unit) with clustered standard errors per unit.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOmitted for visual clarity.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nEstimated total impact calculated as: $$\\Delta \\text{rides} = (e^\\beta - 1) * E(\\text{rides}_{it} | \\text{Near DNC}_i * \\text{Before DNC}_t) * \\sum{\\text{Near DNC}_i * \\text{During DNC}_t}$$ \u0026#160;\u0026#x21a9;\u0026#xfe0e; ","permalink":"https://eric-mc2.github.io/projects/dnc-effect-results/","tags":["data science","econometrics","panel data"],"title":"DNC Transit Effect? (Part 2)"},{"categories":null,"contents":"View app on Observable Cloud\nThis was an interview take-home project. The prompt was \u0026ldquo;Make a dashboard for a school district leader, comparing standardized test scores (3rd grade reading level meets grade level) to household income and poverty at the zip code level. Don\u0026rsquo;t do statistical analysis.\u0026rdquo; Links were provided to school-level test scores, school locations, and zip-level Census ACS data.\nFeatures Search by zip or school district name. Find similar communities based on economic factors (k nearest neighbors). Highlight community of interest\u0026rsquo;s rank (state-wide and compared to similar communities). Designed to compare test scores and economic factors without implying causality. Demo ","permalink":"https://eric-mc2.github.io/projects/3rd-grade-reading/","tags":["data science","data visualization","storytelling","dashboard","Observable Framework","javascript"],"title":"Texas School Reading Scores"},{"categories":null,"contents":"527 Explorer Network visualization of ProPublica\u0026rsquo;s 527 Explorer, built with Observable Framework.\nDemo (so far \u0026hellip;)\nWhy make this? This project is in part, an extension of my prior research into nonprofit donations networks.\nI would like to write my own opinionated d3.js layout that deals with the scale-free and core-periphery characteristics of these networks. Having code to query and visualize a donations dataset was a first step.\nI also have ambitions to create better looking interactive visualizations, and was curious by the interactivity provided by Observable.\n","permalink":"https://eric-mc2.github.io/projects/527-explorer/","tags":["data science","money in politics","d3","typescript"],"title":"527 Network Explorer"},{"categories":null,"contents":"A few weeks ago I fumbled an interview question on statistical modeling. That hurt my pride, so I decided to take on a little modeling project.\nWith all the local hubbub around the DNC security perimeter, I remember mostly staying home to avoid getting snarled in traffic. This got me wondering how the rest of the city responded to the convention. How did the visiting delegates fare? Did they use public transit? (Chicago was picked to highlight its transportation and infrastructure after all.)\nDoes the exogeneous shock of the 2024 Democratic National Convention in Chicago induce significant impacts on public transit usage?\nOn face value I expect public transit ridership to increase due to the DNC. This is because the Democratic coalition generally lives in urban areas, supports \u0026ldquo;green\u0026rdquo; policy, government spending, etc.\nDisclaimer: I don\u0026rsquo;t really have a stake in whether Democrats in general use transit or not, but I like this setup for a few reasons:\nif AN effect is found, that is interesting (statistics are designed to be cautious about stating effects) if NO effect is found, that is also interesting (it suggests the economic impact was smaller than reported, or that delegates took Ubers!) if the whole setup is not statistically valid, I can practice critiquing my own bad statistics (poorly tested statistics can have lasting effects on society!) At First Glance Can we even test this? Is the DNC a big enough event compared to normal commuting patterns?\nThe official DNC impact report says 50,000 delegates visited and spent $58.7M outside of the convention within Chicago. The city-wide average daily CTA ridership is 790,000 +/- 140,000 so it would be hard to detect these extra rides against the normal rhythm of the city.\nBut we know convention event is localised to the United Center and McCormick place. When we only look at CTA routes that pass through these areas, the average daily ridership is 325,000 +/- 52,000. This breaks down to 136,000 bus and 189,000 train rides. So the anticipated impact is getting more noticeable.\nSelecting only nearby train stations (ridership data is not available per bus stop), average daily boardings are 11,000. Any fraction of those 50,000 delegates ought to make a huge spike in this context.\nWhat are the delegates\u0026rsquo; other travel options? Walking, driving, Metra, biking, and rideshares. Data is available for the latter: there are 6,000 and 33,000 average daily bike and rideshare trips in areas near the United Center and McCormick place.\nWith these totals in mind, I\u0026rsquo;d expect the DNC delegation to significantly increase ridership nearby the convention centers, no matter what transit mode they choose.\nThe data The City of Chicago provides daily ridership totals for train (CTA), bus (CTA), rideshare (uber, lyft, etc), and bikeshare (Divvy). The original data is published at varying spatial aggregations:\nTrain: per station Bike: per station Bus: per route Rideshares: per tract / community area I unified these sources into a panel dataset, keeping the nominal spatial unit of each transit mode1. Unfortunately I cannot include busses in this analysis because the data is not granular enough to identify rides near the convention centers.\nTreatment Zone The next step is to label transit rides as near or not near the convention centers.\nConvention Centers\nI use the City of Chicago buildings shapefile to find the footprint of the United Center and McCormick Place, the two official sites of the DNC. Proximity to these locations will constitute the \u0026ldquo;treatment\u0026rdquo; group in the model.\nI compute a buffer around each building and find all intersecting transit stations, routes, and tracts. For robustness, I tested 400m, 800m, and 1600m buffers. To pick which catchement size to use, I was forced to choose one mile buffer, because the smaller sizes had too few stations and routes nearby.\nFig: Transit serving convention areas. To test the sensitivity of the buffer, I modeled:\n$$\\text{rides}_i \\sim \\text{within 400m}_i + \\text{within 800m}_i + \\text{within 1600m}_i$$ Average rides per spatial unit varied irregularly as the buffer size increased, meaning results are very sensitive to the buffer size.\nUsing Community Areas vs Tracts\nI chose to use tract-level data for rideshares. Here\u0026rsquo;s my reasoning. Tracts mean:\n‚úÖ a larger sample size (more units of observation) ‚õîÔ∏è more noise (smaller units =\u0026gt; more variation across units/time) ‚úÖ less bias (smaller units =\u0026gt; 50% less spatial non-compliance on edge of buffer) ‚õîÔ∏è fewer rides (smaller units =\u0026gt; 23% more privacy redactions) Overall the attenuation bias seemed like the most important consideration2.\nFig: Intersected community areas Time-like Features Conference Dates\nThe DNC itself occurs between August 19-22. This constitutes the \u0026ldquo;treatment period\u0026rdquo; in the model34.\nDay of the Week\nCommuting patterns have strong weekday/weekend polarity. CTA ridership is drastically higher on weekdays, while on weekends Uber ridership is higher.\nDue to the restricted time frame of the DNC, I drop observations from Friday - Sunday. (Ridership on these days doesn\u0026rsquo;t convey any information about the \u0026ldquo;treatment\u0026rdquo; because the DNC only occurs on Monday - Thursday.)\nOther Spatial Features Transit Density\nI coded the distance from the tract centroid to the nearest train, bus, and bike stop. These distances will help control for varying transit density, commercial density, and transit mode preferences.\n$$ \\sim \\beta_1 d(\\text{unit}_i, \\text{train}) + \\beta_2 d(\\text{unit}_i, \\text{bus}) + \\beta_3 d(\\text{unit}_i, \\text{bike}) $$ Location\nI include the stop/tract centroid longitude and latitude as a quadratic term in the model to help control for basic city-wide spatial variation:\n$$ \\sim \\beta_1\\text{lon} + \\beta_2\\text{lat} + \\beta_3\\text{lon}*\\text{lat} + \\beta_4\\text{lon}^2 + \\beta_5\\text{lat}^2 $$ (For numeric stability, I scale lon/lat to zero-mean unit variance.)\nThe Sample Temporal Aggregation\nI keep observations at the daily level. Aggregating to weekly would pull non-DNC dates into the treatment period, attenuating the treatment effect.\nTime Frame\nI use data from the summer months, June, July, and August. Restricting the data to this set makes sense for two reasons. First, I avoid needing to incorporate more complex modeling of seasonality. Second, transit is on a steady rebound reaching 65% of pre-pandemic levels. Though I do model a linear time covariate, the rebound suggests that disaggregated transit usage is in flux as commuter preferences and capacity continue to adjust. Therefore, previous summers are not a good baseline.\nBoardings vs Trips\nCTA train and bus data only provides the locations where riders board transit, not where they exit. Bike and rideshare data provides per-trip board and exit locations. For strict parity, I ought to drop exit data, but I won\u0026rsquo;t. Keeping it gives a fuller picture of transit usage56.\nPre-Regression Checks Statistical Power\nBefore running the regression, I want to do a slightly more rigorous version of the time series gut check. Looking at the distribution of daily ridership let\u0026rsquo;s ask: what is the minimum number of additional rides to significantly change the mean? First I compute the sample variance and standard error7, SE, to get the minimum detectable effect size:\n$$ \\text{MDE} = (z(1-\\alpha / 2) + z(\\beta)) * SE $$ Multiplying the MDE by the number of (treated) observations yields the minimum detectable total \u0026ldquo;shock\u0026rdquo; to the system.\nTransit Mean Rides per Unit per Day (SD) Minimum Detectable Change Minimum Additional Rides bike 50.3 (17.5) +151% 10235 train 1482.0 (392.7) +227% 58655 uber 753.1 (583.5) +142% 55036 Let\u0026rsquo;s interpret the first row of this table:\na bike rack near the DNC will serve an average of 50 rides per day ridership would need to increase by 151% to be statistically significant this translates to 954 extra rides over the course of the DNC note: column 1 cannot simply be multiplied by column 2 to produce column 3 (because I log-transform ridership for the model) Corraborating the first time series plot, if a decent fraction of delegates take transit, this regression design should be able to detect an effect.\nSelection Balance\nHere is a balance table for the three separate transit modes:\nNot Near DNC Near DNC P-Value train stations 114 8 station-days 6270 420 daily rides, mean (SD) 2718.4 (2337.9) 1482.8 (995.9) \u0026lt;0.001 log(daily rides), mean (SD) 7.6 (0.8) 6.8 (1.8) \u0026lt;0.001 bus_distance, mean (SD) 305.0 (1472.7) 165.3 (167.7) 0.354 bike_distance, mean (SD) 387.6 (1535.0) 288.5 (281.6) 0.573 sqrt(area), mean (SD) 2521.0 (3723.0) 3123.8 (3564.8) 0.657 lat, mean (SD) -0.1 (0.9) -0.3 (0.2) 0.027 long, mean (SD) -0.0 (1.0) 0.2 (0.4) 0.296 \u0001 bike docks 1459 47 dock-days 42316 2568 daily rides, mean (SD) 46.0 (67.7) 49.5 (34.4) \u0026lt;0.001 log(daily rides), mean (SD) 2.8 (1.6) 3.7 (0.8) \u0026lt;0.001 train_distance, mean (SD) 5007.3 (5396.5) 1056.5 (1240.4) \u0026lt;0.001 bus_distance, mean (SD) 146.2 (264.4) 105.6 (73.5) 0.002 sqrt(area), mean (SD) 666.9 (1009.0) 655.0 (420.6) 0.859 lat, mean (SD) -0.4 (1.3) -0.3 (0.2) 0.248 long, mean (SD) -0.3 (1.3) 0.4 (0.5) \u0026lt;0.001 \u0001 uber tracts 1196 40 tract-days 52995 2189 daily rides, mean (SD) 213.8 (802.3) 737.9 (1714.9) \u0026lt;0.001 log(daily rides), mean (SD) 3.8 (1.7) 5.5 (1.5) \u0026lt;0.001 train_distance, mean (SD) 15886.8 (19731.6) 2293.7 (1027.7) \u0026lt;0.001 bus_distance, mean (SD) 9390.3 (17842.2) 503.0 (313.2) \u0026lt;0.001 bike_distance, mean (SD) 10707.4 (18691.3) 889.5 (504.9) \u0026lt;0.001 sqrt(area), mean (SD) 1134.0 (691.0) 784.4 (320.0) \u0026lt;0.001 lat, mean (SD) -0.1 (1.4) -0.1 (0.2) 0.153 long, mean (SD) -0.5 (1.5) 0.5 (0.3) \u0026lt;0.001 The imbalance in ridership, transit service density, and unit size corroborate the selection effect at play: the convention sites were chosen for their ability to accomodate lots of visitors (duh). These sites are unlike the rest of Chicago. Luckily with the diff-in-diff framework, the convention sites partially control for themselves. Is that enough to believe in this model? I\u0026rsquo;ll come back to this question later.\nRegression This post is already long. I\u0026rsquo;ll present the formal regression model and results in the next post.\nFootnotes Given this approach I will not be able to compare effect sizes between transit modes. Unifying these data into one model is not trivial. To start, the construct we are interested in is travel origins. The best we have are transit origins, but these are not the same \u0026ndash; people need to walk the \u0026ldquo;last mile\u0026rdquo; from their true origin to the nearest station, potentially crossing census tracts. Second, transit modes are substitutes: modeling them together probably requires some kind of structural equation model to handle simultaneity.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nEven at the tract level, 50% of \u0026ldquo;nearby\u0026rdquo; tract land area is outside the buffer. As a robustness check, I should vary the spatial intersection threshold from 0% to 100% and test the sensitivity of the results.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFor a robustness check I should add a 1 or 2 day buffer (travel/tourism days).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSee placebo test.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNote this mechanically doubles the ridership levels of bike/rideshare vs train. I account for this by running separate models per transit mode, and by reporting percentage changes instead of level changes.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIf delegates prefer to commute to the DNC via train and commute back via Uber, I\u0026rsquo;d only observe the return trips. This complicates comparisons between transit modes.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis was more complicated than I expected. The panel is unbalanced in two ways, which would underestimate the standard error if unaccounted for. First, some units have as much as 12x more observations than other units, due to data availability. Second, I\u0026rsquo;ve included 15x more non-DNC days as DNC days, in order to improve the baseline estimates. Instead of the typical \\(SE = \\sigma / \\sqrt n\\) calculation, I use the pooled \\(SE = \\sigma \\sqrt{\\sum{1/n_i}}\\).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://eric-mc2.github.io/projects/dnc-effect/","tags":["data science","econometrics","panel data"],"title":"Do Democrats Really Support Public Transit?"},{"categories":null,"contents":"This page describes the steps I took to create a panel dataset of ridership across different Chicago transit systems. I did this to practice econometric modeling.\nTransit modes and data sources Included modes\nTrain, Bus, Rideshares - from the City of Chicago (used Socrata API and sodapy python package) Bikeshares aka Divvy Bikes - from Lyft (used s3fs python package to access public Amazon S3 bucket) Excluded modes\nMetra commuter rail (public data is available but too coarsely aggregated) Pace city-suburb bus link (have not checked availability) Walking / Biking (vendored data is available \u0026ndash; ie cell phone location data \u0026ndash; but would be hard to isolate from other transit modalities) Driving (public data is available \u0026ndash; ie road segment usage \u0026ndash; but haven\u0026rsquo;t seriously looked yet) Extra data\nGeographic boundaries - from US Census Bureau (used pygris python package) Landmark boundaries - also from City of Chicago ETL Pipeline Major steps in processing this data:\nExtract Get raw unit-level data (i.e. train stations, bus lines) Get raw ridership time series data Transform Account for schema drift in each data source Merge unit info and time series data Compute and add covariates for regression Merge Merge transit modalities at different spatial and time aggregations Choose Choose best aggregation for regression modeling ETL Illustration Some datasets were better organized than others. The easier ones only required:\nselecting spatial and time range to query reading data documentation figuring out which are the table primary and foreign keys light conversion of data and object types In the simplest of cases, here\u0026rsquo;s all the code we need to query and arrange a panel:\nfrom src.data.cta import (ChiClient, BUS_ROUTES_TABLE, BUS_RIDERSHIP_TABLE) from src.data.gis import WORLD_CRS from shapely.geometry import shape # Query static info about bus routes client = ChiClient(60) bus_routes = client.get_all(BUS_ROUTES_TABLE, select=\u0026#34;the_geom, route, name\u0026#34;) bus_routes = (bus_routes .assign(geometry = bus_routes[\u0026#39;the_geom\u0026#39;].apply(shape)) .drop(columns=\u0026#39;the_geom\u0026#39;) .pipe(gpd.GeoDataFrame, crs=WORLD_CRS)) # Query daily rides per route data_start = \u0026#34;2024-01-01T00:00:00\u0026#34; data_end = \u0026#34;2024-08-31T23:59:59\u0026#34; bus_rides = client.get_all(BUS_RIDERSHIP_TABLE, select=\u0026#34;route,date,daytype,rides\u0026#34;, where=f\u0026#34;date between \u0026#39;{data_start}\u0026#39; and \u0026#39;{data_end}\u0026#39;\u0026#34;) bus_panel = bus_rides.merge(bus_routes, how=\u0026#39;left\u0026#39;, on=\u0026#39;route\u0026#39;) Let\u0026rsquo;s plot these routes, colored by ridership:\nAfter getting the direct transit data, I coded a few extra features such as census tract and population, which could serve as useful regression controls later on.\nimport pygris # CTA routes extend outside chicago into cook cty. tracts = pygris.tracts(state=\u0026#39;IL\u0026#39;, county=\u0026#39;cook\u0026#39;, cb=True, year=2020, cache=False) tracts = tracts[[\u0026#39;GEOID\u0026#39;,\u0026#39;geometry\u0026#39;]] tracts[\u0026#39;GEOID\u0026#39;] = pd.to_numeric(tracts[\u0026#39;GEOID\u0026#39;]) def code_tract(gdf, tracts_gdf) -\u0026gt; pd.Series: \u0026#34;\u0026#34;\u0026#34;Spatial join points to census tract\u0026#34;\u0026#34;\u0026#34; codes = (gdf .filter([\u0026#39;geometry\u0026#39;]) .to_crs(tracts_gdf.crs) .sjoin(tracts_gdf, how=\u0026#39;left\u0026#39;, predicate=\u0026#39;within\u0026#39;)) return pd.concat([gdf, codes[\u0026#39;GEOID\u0026#39;].rename(\u0026#39;tract\u0026#39;), axis=1]) bus_stops = bus_stops.pipe(code_tract) Now we know the immediate population served by each bus line.\nSince ridership is reported at different levels of granularity for each transit modality, I had to be careful how to spatially aggregate the data. My goal is to test whether local events, such as football games, have a measurable effect on transit. Associating transit stations to points of interest is straightforward \u0026ndash; each point is an unambiguous distance away. Associating shapes such as bus routes or census tracts is more ambiguous \u0026ndash; do we measure the distance to the closest bus stop or the furthest? The data doesn\u0026rsquo;t tell us whether riders travel short or far distances along these routes. This ambiguity can create inaccurate regression estimates.\nTrain: provides daily boarding counts per station. we do not know where each passenger exited, nor which direction or train route they took. Bus: provides daily boardings counts per route. we do not know how this breaks down per bus stop, nor the direction or stop passengers exited at. Bike: provides trip-level data: timestamped station pickups and dropoffs per ride. Bike: provides trip-level data: timestamped pickup and dropoffs per ride. pickup and dropoff points are anonymized to their containing census tract. Modality Train Bus Bike Uber Time granularity Daily Daily Minute Minute Direction of travel No No No No Point of departure Yes No Yes No Line of departure Inferred Yes No No Area of departure Yes No Yes Yes Point of arrival No No Yes No Line of arrival Inferred Yes No No Area of arrival Yes No Yes Yes I aggregated three comparable panels:\nPoint: train, bike Line: train, bus Area: train, bike, uber Special Challenges Divvy Locations The divvy historical data spans from 2013 to 2024. Over these years, the published schema has changed several times. This led to annoying, but forgiveable issues to work around:\nBatch Size: Some years are published in monthly batches, others quarterly. This isn\u0026rsquo;t an issue for data modeling, since we just concatenate, but it requires extra logic for properly discovering and enumerating the available data.\nInconsistent column names: Also not an issue for modeling, since I just rename them, but requires manually curating a mapping of {old name -\u0026gt; new name} .\nNormalized vs Denormalized: Some years contain data in \u0026ldquo;database normalized\u0026rdquo; format with separate stations and rides tables, linked by a common station_id key. Other years only contain a single de-normalized/merged rides table, with station info included.\nBut the divvy data contained two way worse problems:\nUnstable station id\u0026rsquo;s\nIn some batches, the station ID is an integer, while in others it is a UUID, representing schema drift. This drift broke the one-to-one property of station IDs: 60% of station IDs were one-to-many: they were associated with up to 4 unique stations.\nFor those not familiar with Chicago streets, these intersections are quite far apart:\nThe standard approach to resolving this issue is to create a column labeling each observation with the batch it came from (eg. \u0026ldquo;2024-Q1\u0026rdquo;) and use (ID, batch) pairs as the new primary key. Although this fixes the one-to-many issue, it breaks the injective property of station IDs, making them many-to-one!\nUnstable station name\nLooking at the data this way, perhaps the station name is a sufficient primary key?\nUnfortunately, station names were also not always one-to-one. For example, the following locations are all purportedly at Buckingham Fountain:\nSimilarly, station names were also not always injective.\nImprecise station locations\nRecent Divvy historical data batches are published as single denormalized CSV\u0026rsquo;s containing trip_start_location and trip_end_location geometries. Maybe the points can be our primary keys?\nAt first glance, there are some ~900,000 unique points. At MOST I should expect a few thousand!\nMaybe this is a floating point precision issue and the points are actually well-clustered around their true station locations. Maybe I can derive a much smaller set of representative locations and snap each point to the closest one.\nAttribute Group-By: I group by attribute (e.g. name, id, batch), compute the centroid of each group, and measure the maximum point-to-centroid distance per group. This quantifies the spread or imprecision in using names, ids, batches as primary keys to actually identify station locations.\nIn code this would look something like:\nTOLERANCE = 1320 # feet def dispersion(x: gpd.GeoSeries, metric: Callable = np.std): # Taking centroid of unique points makes us more sensitive to outlier mis-labeled data when # using standard deviation as metric. When using max it makes no difference. x = x.drop_duplicates() c = MultiPoint(x.values).centroid radius = metric(x.distance(c)) diam = radius * 2 # very rough return diam max_spread = bike_rides.to_crs(LOCAL_CRS) .groupby(\u0026#39;name\u0026#39;)[\u0026#39;geometry\u0026#39;] .transform(lambda x: dispersion(x, np.max)) clusterable = max_spread \u0026lt; TOLERANCE Based on this method, ~106,000 points can be reduced down to ~1,00 representative points, but the remaining ~786,000 points are too dispersed to confidently say the refer to the same point.\nSpatial Clustering: As seen above, these ID columns are not necessarly bijective, meaning they are over-specified. Maybe I should group the points purely by their location, and ignore their IDs.\nThere are many ways to cluster points. I\u0026rsquo;ll use an intuitive way as follows:\nDraw a buffer around each point (66ft is the typical Chicago street width) Find all buffers that intersect (via unary union) Each unioned buffer shape represents a candidate cluster Build spatial index on these clusters For each point, determine which cluster it belongs to (via spatial index query) Break apart clusters that are: Too dispersed Have heterogenous attributes (e.g. station names) This method affirms the outcome of the attribute clustering performed above. The 20% of data that is tightly spatially grouped do indeed represent unique stations. The other 80% of the data can be unioned into clusters that contain up to 7818 points.\nThe buffer union method suffers from a transitivity issue:\n$$ \\text{dist}(A,B) \u003c d, \\text{dist}(B,C) \u003c d \\not \\Rightarrow \\text{dist}(A,C) \u003c d $$ We can see that step 5 helps mitigate this issue.\nI have to conclude that these points actually represent user locations when they start/stop trips on their smartphone apps. The drift represents a combination of GPS uncertainty among tall downtown buildings, and physical user displacement from the actual station when they interact with the app.\nResolution with GBFS\nAs a last resort, I looked at and decided to merge the live GBFS feed. This dataset is intended for app developers, and shows real-time station availability and such. The problem is it\u0026rsquo;s live and doesn\u0026rsquo;t span the entire historical data range. Given the schema drift issues presented above, I had no confidence the data was merge-able with anything but 2024 data. But since I only strictly need 2024 data for the models I plan to run, this limitation is acceptable.\nProblem solved!\nSocrata to Data Frames As far as I could tell, the Socrata SoQL API returned all data as lists of lists of strings. Since I was making a lot of Socrata calls, I extended the Socrata client like so:\nfrom sodapy import Socrata import pandas as pd class ChiClient(Socrata): def __init__(self, timeout: int): super().__init__( \u0026#34;data.cityofchicago.org\u0026#34;, app_token=None, timeout=timeout) def get(self, resource_id: str, **params) -\u0026gt; pd.DataFrame: \u0026#34;\u0026#34;\u0026#34;Collects Socrata response into data frame.\u0026#34;\u0026#34;\u0026#34; data = super().get(resource_id, **params) df = pd.DataFrame.from_records(data) df = self.fix_coltypes(df, resource_id) return df def fix_coltypes(self, df: pd.DataFrame, resource_id: str): \u0026#34;\u0026#34;\u0026#34;Coerce string data into other dtypes\u0026#34;\u0026#34;\u0026#34; coltypes = self.get_coltypes(resource_id) for col in df.columns: if col not in coltypes.keys(): # Column was transformed e.g. SELECT COUNT(col) # or renamed e.g. SELECT col AS alias continue elif coltypes[col] == \u0026#39;calendar_date\u0026#39;: df[col] = pd.to_datetime(df[col]) elif coltypes[col] == \u0026#39;number\u0026#39;: df[col] = pd.to_numeric(df[col]) return df def get_coltypes(self, resource_id: str): \u0026#34;\u0026#34;\u0026#34;Query basic table metadata\u0026#34;\u0026#34;\u0026#34; meta = self.get_metadata(resource_id) colnames = [c[\u0026#39;fieldName\u0026#39;] for c in meta[\u0026#39;columns\u0026#39;]] coltypes = [c[\u0026#39;dataTypeName\u0026#39;] for c in meta[\u0026#39;columns\u0026#39;]] coltypes = {c: ct for c,ct in zip(colnames, coltypes)} return coltypes Usage comparison:\nIn [1]: TOTAL_RIDERSHIP_TABLE = \u0026#34;6iiy-9s97\u0026#34; In [2]: soc_client = Socrata(\u0026#34;data.cityofchicago.org\u0026#34;, app_token=None, timeout=60) In [3]: chi_client = ChiClient(60) In [4]: original_data = soc_client.get(TOTAL_RIDERSHIP_TABLE, limit=5) In [5]: pretty_data = chi_client.get(TOTAL_RIDERSHIP_TABLE, limit=5) In [6]: print(\u0026#34;Original response:\u0026#34;, original_data, sep=\u0026#34;\\n\u0026#34;) Original response: [{\u0026#39;service_date\u0026#39;: \u0026#39;2001-01-01T00:00:00.000\u0026#39;, \u0026#39;day_type\u0026#39;: \u0026#39;U\u0026#39;, \u0026#39;bus\u0026#39;: \u0026#39;297192\u0026#39;, \u0026#39;rail_boardings\u0026#39;: \u0026#39;126455\u0026#39;, \u0026#39;total_rides\u0026#39;: \u0026#39;423647\u0026#39;}, {\u0026#39;service_date\u0026#39;: \u0026#39;2001-01-02T00:00:00.000\u0026#39;, \u0026#39;day_type\u0026#39;: \u0026#39;W\u0026#39;, \u0026#39;bus\u0026#39;: \u0026#39;780827\u0026#39;, \u0026#39;rail_boardings\u0026#39;: \u0026#39;501952\u0026#39;, \u0026#39;total_rides\u0026#39;: \u0026#39;1282779\u0026#39;}, {\u0026#39;service_date\u0026#39;: \u0026#39;2001-01-03T00:00:00.000\u0026#39;, \u0026#39;day_type\u0026#39;: \u0026#39;W\u0026#39;, \u0026#39;bus\u0026#39;: \u0026#39;824923\u0026#39;, \u0026#39;rail_boardings\u0026#39;: \u0026#39;536432\u0026#39;, \u0026#39;total_rides\u0026#39;: \u0026#39;1361355\u0026#39;}, {\u0026#39;service_date\u0026#39;: \u0026#39;2001-01-04T00:00:00.000\u0026#39;, \u0026#39;day_type\u0026#39;: \u0026#39;W\u0026#39;, \u0026#39;bus\u0026#39;: \u0026#39;870021\u0026#39;, \u0026#39;rail_boardings\u0026#39;: \u0026#39;550011\u0026#39;, \u0026#39;total_rides\u0026#39;: \u0026#39;1420032\u0026#39;}, {\u0026#39;service_date\u0026#39;: \u0026#39;2001-01-05T00:00:00.000\u0026#39;, \u0026#39;day_type\u0026#39;: \u0026#39;W\u0026#39;, \u0026#39;bus\u0026#39;: \u0026#39;890426\u0026#39;, \u0026#39;rail_boardings\u0026#39;: \u0026#39;557917\u0026#39;, \u0026#39;total_rides\u0026#39;: \u0026#39;1448343\u0026#39;}] In [7]: print(\u0026#34;DataFrame response:\u0026#34;, pretty_data, sep=\u0026#34;\\n\u0026#34;) DataFrame response: service_date day_type bus rail_boardings total_rides 0 2001-01-01 U 297192 126455 423647 1 2001-01-02 W 780827 501952 1282779 2 2001-01-03 W 824923 536432 1361355 3 2001-01-04 W 870021 550011 1420032 4 2001-01-05 W 890426 557917 1448343 ","permalink":"https://eric-mc2.github.io/projects/transit-panel/","tags":["data science","open data","urban science","Socrata","Amazon S3","geopandas"],"title":"Chicago Transit Ridership Panel"},{"categories":null,"contents":"How does private money flow to police departments, and what do these donations reveal about power and accountability in public safety?\nAlthough previous studies have focused on police foundations moving tens of millions of dollars, the authors‚Äô research uncovers a broader, more complex ecosystem of private police financial support. Using a unique data license from GuideStar Candid‚Äîall nonprofit organization tax returns from 2014 to 2019‚Äîthe authors describe a larger and integrated world of organizations moving private money to police. The authors introduce the concept of police finance organizations: private entities that supply material resources to police departments without facing the same levels of public accountability, regulations, and transparency requirements as the police departments themselves. This exploratory data analysis and social network methods revealed three types of these organizations: (1) connectors, which distribute resources across multiple police departments; (2) boosters, which donate exclusively to one police department; and (3) havens, which publicly and materially support police without officially reporting these gifts. Overall, the authors provide the most complete description of the vast and underestimated structure of private donations, redefining the understanding of private-public intersections in policing and raising new questions about accountability in public safety practice.\n","permalink":"https://eric-mc2.github.io/publications/shachter-structure/","tags":["social media analysis","network analysis","SNA","NLP"],"title":"The Social Structure of Private Donations to Police"}]